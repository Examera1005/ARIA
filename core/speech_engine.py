"""
üé§ Moteur de Reconnaissance et Synth√®se Vocale
============================================

Syst√®me avanc√© pour :
- Reconnaissance vocale en fran√ßais (multi-moteurs)
- Synth√®se vocale naturelle
- D√©tection de mots-cl√©s d'activation
- Filtrage du bruit et am√©lioration audio
"""

import asyncio
import speech_recognition as sr
import pyttsx3
import pyaudio
import wave
import json
import logging
import numpy as np
from typing import Optional, Dict, Any, List
from pathlib import Path
import threading
import queue
import time

# Tentative d'import des moteurs avanc√©s
try:
    import vosk
    VOSK_AVAILABLE = True
except ImportError:
    VOSK_AVAILABLE = False

try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

try:
    import azure.cognitiveservices.speech as speechsdk
    AZURE_AVAILABLE = True
except ImportError:
    AZURE_AVAILABLE = False

class SpeechEngine:
    """Moteur de reconnaissance et synth√®se vocale"""
    
    def __init__(self, config):
        """Initialiser le moteur vocal"""
        self.config = config
        self.logger = logging.getLogger("ARIA.Speech")
        
        # Moteurs de reconnaissance
        self.recognizer = sr.Recognizer()
        self.microphone = sr.Microphone()
        
        # Moteur de synth√®se
        self.tts_engine = None
        
        # Mod√®les offline (Vosk)
        self.vosk_model = None
        self.vosk_recognizer = None
        
        # Param√®tres audio
        self.sample_rate = 16000
        self.chunk_size = 1024
        self.audio_queue = queue.Queue()
        
        # Mots-cl√©s d'activation
        self.wake_words = [
            "aria", "arya", "area", "aria √©coute", "√°ria",
            "hey aria", "ok aria", "salut aria"
        ]
        
        # √âtat
        self.listening = False
        self.speaking = False
        self.wake_word_detected = False
        
        # Configuration du recognizer
        self.recognizer.energy_threshold = 4000
        self.recognizer.dynamic_energy_threshold = True
        self.recognizer.pause_threshold = 0.8
        self.recognizer.operation_timeout = None
        
        self.logger.info("üé§ Moteur vocal initialis√©")
    
    async def initialize(self):
        """Initialiser tous les composants vocaux"""
        self.logger.info("üöÄ Initialisation du moteur vocal...")
        
        try:
            # Initialiser la synth√®se vocale
            await self.init_tts()
            
            # Initialiser les moteurs de reconnaissance
            await self.init_recognition_engines()
            
            # Calibrer le microphone
            await self.calibrate_microphone()
            
            # Tester les composants
            await self.test_components()
            
            self.logger.info("‚úÖ Moteur vocal initialis√© avec succ√®s")
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur initialisation moteur vocal: {e}")
            raise
    
    async def init_tts(self):
        """Initialiser la synth√®se vocale"""
        try:
            self.tts_engine = pyttsx3.init()
            
            # Configuration de la voix
            voices = self.tts_engine.getProperty('voices')
            
            # Chercher une voix fran√ßaise
            french_voice = None
            for voice in voices:
                if 'french' in voice.name.lower() or 'fr' in voice.id.lower():
                    french_voice = voice
                    break
            
            if french_voice:
                self.tts_engine.setProperty('voice', french_voice.id)
                self.logger.info(f"üó£Ô∏è Voix fran√ßaise s√©lectionn√©e: {french_voice.name}")
            else:
                self.logger.warning("‚ö†Ô∏è Aucune voix fran√ßaise trouv√©e, utilisation de la voix par d√©faut")
            
            # Configuration des param√®tres
            self.tts_engine.setProperty('rate', self.config.tts_rate)  # Vitesse
            self.tts_engine.setProperty('volume', self.config.tts_volume)  # Volume
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur initialisation TTS: {e}")
            raise
    
    async def init_recognition_engines(self):
        """Initialiser les moteurs de reconnaissance"""
        engines_loaded = []
        
        # 1. Moteur offline Vosk
        if VOSK_AVAILABLE and self.config.use_vosk:
            try:
                model_path = Path(self.config.vosk_model_path)
                if model_path.exists():
                    self.vosk_model = vosk.Model(str(model_path))
                    self.vosk_recognizer = vosk.KaldiRecognizer(
                        self.vosk_model, 
                        self.sample_rate
                    )
                    engines_loaded.append("Vosk (offline)")
                    self.logger.info("‚úÖ Moteur Vosk charg√©")
                else:
                    self.logger.warning(f"‚ö†Ô∏è Mod√®le Vosk non trouv√©: {model_path}")
            except Exception as e:
                self.logger.error(f"‚ùå Erreur chargement Vosk: {e}")
        
        # 2. Google Speech Recognition
        if self.config.use_google_sr:
            engines_loaded.append("Google Speech Recognition")
            self.logger.info("‚úÖ Google Speech Recognition activ√©")
        
        # 3. Azure Speech Services
        if AZURE_AVAILABLE and self.config.azure_speech_key:
            try:
                self.azure_speech_config = speechsdk.SpeechConfig(
                    subscription=self.config.azure_speech_key,
                    region=self.config.azure_speech_region
                )
                self.azure_speech_config.speech_recognition_language = "fr-FR"
                engines_loaded.append("Azure Speech Services")
                self.logger.info("‚úÖ Azure Speech Services configur√©")
            except Exception as e:
                self.logger.error(f"‚ùå Erreur Azure Speech: {e}")
        
        # 4. OpenAI Whisper
        if OPENAI_AVAILABLE and self.config.openai_api_key:
            engines_loaded.append("OpenAI Whisper")
            self.logger.info("‚úÖ OpenAI Whisper activ√©")
        
        self.logger.info(f"üé§ Moteurs de reconnaissance charg√©s: {', '.join(engines_loaded)}")
    
    async def calibrate_microphone(self):
        """Calibrer le microphone pour le bruit ambiant"""
        try:
            self.logger.info("üéôÔ∏è Calibration du microphone...")
            
            with self.microphone as source:
                # Ajuster pour le bruit ambiant
                self.recognizer.adjust_for_ambient_noise(source, duration=2)
            
            self.logger.info(f"‚úÖ Microphone calibr√© (seuil: {self.recognizer.energy_threshold})")
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur calibration microphone: {e}")
    
    async def test_components(self):
        """Tester les composants vocaux"""
        try:
            # Test synth√®se vocale
            if self.config.test_on_startup:
                await self.speak("Test du syst√®me vocal ARIA.", wait=True)
            
            self.logger.info("‚úÖ Composants vocaux test√©s")
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur test composants: {e}")
    
    async def listen(self, timeout: float = None) -> Optional[sr.AudioData]:
        """√âcouter et capturer l'audio"""
        try:
            with self.microphone as source:
                # √âcouter l'audio
                audio_data = self.recognizer.listen(
                    source,
                    timeout=timeout or self.config.listen_timeout,
                    phrase_time_limit=self.config.phrase_time_limit
                )
                
                return audio_data
        
        except sr.WaitTimeoutError:
            # Timeout normal, pas d'erreur
            return None
        except Exception as e:
            self.logger.error(f"‚ùå Erreur capture audio: {e}")
            return None
    
    async def transcribe(self, audio_data: sr.AudioData) -> Optional[str]:
        """Transcrire l'audio en texte"""
        transcriptions = []
        
        # Essayer plusieurs moteurs pour plus de pr√©cision
        engines = []
        
        # 1. Vosk (offline, rapide)
        if self.vosk_recognizer:
            engines.append(("vosk", self.transcribe_vosk))
        
        # 2. Google (online, pr√©cis)
        if self.config.use_google_sr:
            engines.append(("google", self.transcribe_google))
        
        # 3. Azure (online, tr√®s pr√©cis)
        if hasattr(self, 'azure_speech_config'):
            engines.append(("azure", self.transcribe_azure))
        
        # 4. OpenAI Whisper (online, excellent)
        if OPENAI_AVAILABLE and self.config.openai_api_key:
            engines.append(("whisper", self.transcribe_whisper))
        
        # Transcrire avec chaque moteur
        for engine_name, transcribe_func in engines:
            try:
                text = await transcribe_func(audio_data)
                if text and len(text.strip()) > 0:
                    transcriptions.append({
                        "engine": engine_name,
                        "text": text.strip().lower(),
                        "confidence": self.estimate_confidence(text)
                    })
                    self.logger.debug(f"üé§ {engine_name}: '{text}'")
            except Exception as e:
                self.logger.error(f"‚ùå Erreur {engine_name}: {e}")
        
        # S√©lectionner la meilleure transcription
        if transcriptions:
            # Trier par confiance
            transcriptions.sort(key=lambda x: x["confidence"], reverse=True)
            best = transcriptions[0]
            
            self.logger.info(f"üéØ Meilleure transcription ({best['engine']}): '{best['text']}'")
            return best["text"]
        
        return None
    
    async def transcribe_vosk(self, audio_data: sr.AudioData) -> Optional[str]:
        """Transcription avec Vosk (offline)"""
        try:
            # Convertir en format requis par Vosk
            audio_np = np.frombuffer(audio_data.get_raw_data(), dtype=np.int16)
            
            if self.vosk_recognizer.AcceptWaveform(audio_np.tobytes()):
                result = json.loads(self.vosk_recognizer.Result())
                return result.get("text", "")
            else:
                result = json.loads(self.vosk_recognizer.PartialResult())
                return result.get("partial", "")
                
        except Exception as e:
            self.logger.error(f"‚ùå Erreur Vosk: {e}")
            return None
    
    async def transcribe_google(self, audio_data: sr.AudioData) -> Optional[str]:
        """Transcription avec Google Speech Recognition"""
        try:
            text = self.recognizer.recognize_google(
                audio_data, 
                language="fr-FR",
                show_all=False
            )
            return text
        except sr.UnknownValueError:
            return None
        except Exception as e:
            self.logger.error(f"‚ùå Erreur Google SR: {e}")
            return None
    
    async def transcribe_azure(self, audio_data: sr.AudioData) -> Optional[str]:
        """Transcription avec Azure Speech Services"""
        try:
            # Convertir l'audio pour Azure
            audio_format = speechsdk.audio.AudioStreamFormat(
                samples_per_second=audio_data.sample_rate,
                bits_per_sample=audio_data.sample_width * 8,
                channels=1
            )
            
            audio_stream = speechsdk.audio.PushAudioInputStream(audio_format)
            audio_config = speechsdk.audio.AudioConfig(stream=audio_stream)
            
            speech_recognizer = speechsdk.SpeechRecognizer(
                speech_config=self.azure_speech_config,
                audio_config=audio_config
            )
            
            # Envoyer les donn√©es audio
            audio_stream.write(audio_data.get_raw_data())
            audio_stream.close()
            
            # Reconna√Ætre
            result = speech_recognizer.recognize_once()
            
            if result.reason == speechsdk.ResultReason.RecognizedSpeech:
                return result.text
            else:
                return None
                
        except Exception as e:
            self.logger.error(f"‚ùå Erreur Azure Speech: {e}")
            return None
    
    async def transcribe_whisper(self, audio_data: sr.AudioData) -> Optional[str]:
        """Transcription avec OpenAI Whisper"""
        try:
            # Sauvegarder temporairement l'audio
            import tempfile
            import os
            
            with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_file:
                tmp_file.write(audio_data.get_wav_data())
                tmp_file_path = tmp_file.name
            
            try:
                # Utiliser l'API Whisper
                client = openai.OpenAI(api_key=self.config.openai_api_key)
                
                with open(tmp_file_path, "rb") as audio_file:
                    transcript = client.audio.transcriptions.create(
                        model="whisper-1",
                        file=audio_file,
                        language="fr"
                    )
                
                return transcript.text
                
            finally:
                # Nettoyer le fichier temporaire
                os.unlink(tmp_file_path)
                
        except Exception as e:
            self.logger.error(f"‚ùå Erreur Whisper: {e}")
            return None
    
    def estimate_confidence(self, text: str) -> float:
        """Estimer la confiance d'une transcription"""
        if not text or len(text.strip()) == 0:
            return 0.0
        
        confidence = 0.5  # Base
        
        # Bonus pour la longueur
        if len(text) > 10:
            confidence += 0.2
        
        # Bonus pour les mots fran√ßais courants
        french_words = ["le", "la", "les", "de", "du", "des", "un", "une", 
                       "et", "ou", "que", "qui", "avec", "dans", "sur", "pour"]
        
        words = text.lower().split()
        french_count = sum(1 for word in words if word in french_words)
        
        if words:
            french_ratio = french_count / len(words)
            confidence += french_ratio * 0.3
        
        return min(confidence, 1.0)
    
    async def detect_wake_word(self, text: str) -> bool:
        """D√©tecter les mots-cl√©s d'activation"""
        if not text:
            return False
        
        text_lower = text.lower()
        
        for wake_word in self.wake_words:
            if wake_word in text_lower:
                self.logger.info(f"üîä Mot-cl√© d√©tect√©: '{wake_word}' dans '{text}'")
                return True
        
        return False
    
    async def speak(self, text: str, wait: bool = False):
        """Synth√®se vocale"""
        if self.speaking:
            return  # √âviter les conflits
        
        self.speaking = True
        
        try:
            self.logger.info(f"üó£Ô∏è Synth√®se: '{text}'")
            
            if wait:
                # Synth√®se synchrone
                self.tts_engine.say(text)
                self.tts_engine.runAndWait()
            else:
                # Synth√®se asynchrone
                def speak_async():
                    self.tts_engine.say(text)
                    self.tts_engine.runAndWait()
                
                thread = threading.Thread(target=speak_async)
                thread.daemon = True
                thread.start()
        
        except Exception as e:
            self.logger.error(f"‚ùå Erreur synth√®se vocale: {e}")
        
        finally:
            self.speaking = False
    
    async def listen_for_wake_word(self) -> bool:
        """√âcouter en permanence pour les mots-cl√©s d'activation"""
        try:
            # √âcoute courte pour wake word
            audio_data = await self.listen(timeout=2.0)
            
            if audio_data:
                # Transcription rapide (uniquement offline)
                if self.vosk_recognizer:
                    text = await self.transcribe_vosk(audio_data)
                    if text and await self.detect_wake_word(text):
                        return True
            
            return False
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur d√©tection wake word: {e}")
            return False
    
    async def listen_command(self) -> Optional[str]:
        """√âcouter une commande compl√®te apr√®s activation"""
        try:
            self.logger.info("üé§ √âcoute d'une commande...")
            
            # √âcoute plus longue pour la commande
            audio_data = await self.listen(timeout=10.0)
            
            if audio_data:
                # Transcription compl√®te avec tous les moteurs
                text = await self.transcribe(audio_data)
                
                if text:
                    self.logger.info(f"üéØ Commande re√ßue: '{text}'")
                    return text
            
            return None
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur √©coute commande: {e}")
            return None
    
    def get_available_engines(self) -> List[str]:
        """Obtenir la liste des moteurs disponibles"""
        engines = []
        
        if VOSK_AVAILABLE and self.vosk_model:
            engines.append("Vosk (offline)")
        
        if self.config.use_google_sr:
            engines.append("Google Speech Recognition")
        
        if AZURE_AVAILABLE and self.config.azure_speech_key:
            engines.append("Azure Speech Services")
        
        if OPENAI_AVAILABLE and self.config.openai_api_key:
            engines.append("OpenAI Whisper")
        
        return engines
    
    async def set_voice_settings(self, rate: int = None, volume: float = None):
        """Modifier les param√®tres de la voix"""
        try:
            if rate is not None:
                self.tts_engine.setProperty('rate', rate)
                self.config.tts_rate = rate
            
            if volume is not None:
                self.tts_engine.setProperty('volume', volume)
                self.config.tts_volume = volume
            
            self.logger.info(f"üîß Param√®tres vocaux mis √† jour: rate={rate}, volume={volume}")
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur modification param√®tres vocaux: {e}")
    
    async def cleanup(self):
        """Nettoyer les ressources"""
        try:
            if self.tts_engine:
                self.tts_engine.stop()
            
            self.logger.info("üßπ Ressources vocales nettoy√©es")
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur nettoyage ressources: {e}")
